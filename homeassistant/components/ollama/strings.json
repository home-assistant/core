{
  "config": {
    "step": {
      "user": {
        "data": {
          "url": "[%key:common::config_flow::data::url%]"
        }
      }
    },
    "abort": {
      "already_configured": "[%key:common::config_flow::abort::already_configured_service%]"
    },
    "error": {
      "invalid_url": "[%key:common::config_flow::error::invalid_host%]",
      "cannot_connect": "[%key:common::config_flow::error::cannot_connect%]",
      "unknown": "[%key:common::config_flow::error::unknown%]"
    }
  },
  "config_subentries": {
    "conversation": {
      "initiate_flow": {
        "user": "Add conversation agent",
        "reconfigure": "Reconfigure conversation agent"
      },
      "entry_type": "Conversation agent",
      "step": {
        "set_options": {
          "data": {
            "model": "Model",
            "name": "[%key:common::config_flow::data::name%]",
            "prompt": "Instructions",
            "llm_hass_api": "[%key:common::config_flow::data::llm_hass_api%]",
            "max_history": "Max history messages",
            "num_ctx": "Context window size",
            "keep_alive": "Keep alive",
            "think": "Think before responding"
          },
          "data_description": {
            "prompt": "Instruct how the LLM should respond. This can be a template.",
            "keep_alive": "Duration in seconds for Ollama to keep model in memory. -1 = indefinite, 0 = never.",
            "num_ctx": "Maximum number of text tokens the model can process. Lower to reduce Ollama RAM, or increase for a large number of exposed entities.",
            "think": "If enabled, the LLM will think before responding. This can improve response quality but may increase latency."
          }
        },
        "download": {
          "title": "Downloading model"
        }
      },
      "abort": {
        "reconfigure_successful": "[%key:common::config_flow::abort::reconfigure_successful%]",
        "entry_not_loaded": "Failed to add agent. The configuration is disabled.",
        "download_failed": "Model downloading failed",
        "cannot_connect": "[%key:common::config_flow::error::cannot_connect%]"
      },
      "progress": {
        "download": "Please wait while the model is downloaded, which may take a very long time. Check your Ollama server logs for more details."
      }
    }
  }
}
